{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsXEBlX1dCkM"
   },
   "source": [
    "# Image classification with Swin Transformers\n",
    "\n",
    "**Author:** [Rishit Dagli](https://twitter.com/rishit_dagli)<br>\n",
    "**Date created:** 2021/09/08<br>\n",
    "**Last modified:** 2021/09/08<br>\n",
    "**Description:** Image classification using Swin Transformers, a general-purpose backbone for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n5ReC2kdCkO"
   },
   "source": [
    "This example implements [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n",
    "by Liu et al. for image classification, and demonstrates it on the\n",
    "[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Swin Transformer (**S**hifted **Win**dow Transformer) can serve as a general-purpose backbone\n",
    "for computer vision. Swin Transformer is a hierarchical Transformer whose\n",
    "representations are computed with _shifted windows_. The shifted window scheme\n",
    "brings greater efficiency by limiting self-attention computation to\n",
    "non-overlapping local windows while also allowing for cross-window connections.\n",
    "This architecture has the flexibility to model information at various scales and has\n",
    "a linear computational complexity with respect to image size.\n",
    "\n",
    "This example requires TensorFlow 2.5 or higher, as well as TensorFlow Addons,\n",
    "which can be installed using the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMhSyRaTdT6H"
   },
   "source": [
    "# 新段落"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeYZlB7zdCkQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKlDB7wY8eHE",
    "outputId": "fab17ab1-fbb6-4e9b-b5f2-585009931af2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in d:\\anaconda3\\envs\\tf\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow_addons) (2.13.3)\n",
      "Requirement already satisfied: geatpy in d:\\anaconda3\\envs\\tf\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from geatpy) (1.21.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from geatpy) (3.5.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib>=3.0.0->geatpy) (3.0.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib>=3.0.0->geatpy) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib>=3.0.0->geatpy) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib>=3.0.0->geatpy) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib>=3.0.0->geatpy) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib>=3.0.0->geatpy) (9.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib>=3.0.0->geatpy) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\envs\\tf\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->geatpy) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons\n",
    "!pip install geatpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b8q0rEmGdCkR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import geatpy as ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jrcxaGdW700i",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_classes = 2\n",
    "# # root = \"C:/Users/wuwul/Desktop/f/f\"\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# def read_image(image_name):\n",
    "#     im = Image.open(image_name)\n",
    "# # #     .convert('L')\n",
    "#     data = np.array(im)\n",
    "#     return data[:,:,:3]\n",
    "\n",
    "# # Generator = ImageDataGenerator()\n",
    "# # train_data = Generator.flow_from_directory(train_root, (100, 100), batch_size=batch_size)\n",
    "# # test_data = Generator.flow_from_directory(test_root, (100, 100), batch_size=batch_size)\n",
    "# # print(train_data)\n",
    "# images = []\n",
    "# labels = []\n",
    "# test = os.listdir(\"C:/Users/wuwul/Desktop/f/f\")\n",
    "# print(test)\n",
    "# for testpath in test:\n",
    "#     for fn in os.listdir(os.path.join(\"C:/Users/wuwul/Desktop/f/f\",testpath)):\n",
    "#         if fn.endswith('.png'):\n",
    "#             fd = os.path.join(\"C:/Users/wuwul/Desktop/f/f\",testpath,fn)\n",
    "# # #             print(fd)\n",
    "#             images.append(read_image(fd))\n",
    "#             labels.append(testpath)\n",
    "# X = np.array(images)\n",
    "# Y = np.array(list(map(int,labels)))\n",
    "\n",
    "\n",
    "# X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X,Y,test_size = 0.3,random_state = 0)\n",
    "# X_train = X_train.astype(np.float32)\n",
    "# Y_train = Y_train.astype(np.float32)\n",
    "\n",
    "    # #optional\n",
    "# print(train_data[0][0][0].shape)\n",
    "# # total 4317 data below to 5 clasess\n",
    "# print(len(train_data)) #4317/batch size\n",
    "# print(len(train_data[0])) #2, 1st image, 2nd is label\n",
    "# #print(train_data[0])\n",
    "# print(len(train_data[0][0])) #1st batch of 10 data\n",
    "# print(len(train_data[0][0][0])) #the image, the vertical\n",
    "# print(len(train_data[0][0][0][0])) #the image, the horizontal\n",
    "# print(len(train_data[0][0][0][0][0])) #the image, RGB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lvLW2j3b700j"
   },
   "outputs": [],
   "source": [
    "# ea.soea_SEGA_templet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZQgPRRCm700j"
   },
   "outputs": [],
   "source": [
    "# ea.Problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_k2NOyC8EOE",
    "outputId": "f1cbd561-4064-47f6-85d7-8166ecef3882"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7shmiwur700j"
   },
   "outputs": [],
   "source": [
    "train_root = \"C:/Users/wuwul/Desktop/Face Mask Dataset 2/Train\"\n",
    "test_root = \"C:/Users/wuwul/Desktop/Face Mask Dataset 2/Train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDo9CZtJ700k",
    "outputId": "bfae05c8-cc7b-4e06-8508-bcc6db29093a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 383 images belonging to 2 classes.\n",
      "Found 383 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "Generator = ImageDataGenerator()\n",
    "train_data = Generator.flow_from_directory(train_root, (224, 224), batch_size=100,classes = ['WithMask','WithoutMask'])\n",
    "test_data =  Generator.flow_from_directory(test_root, (224, 224), batch_size=100,classes = ['WithMask','WithoutMask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XXJvcyPR700l"
   },
   "outputs": [],
   "source": [
    "m = train_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RKoTgyj7700l",
    "outputId": "c225df44-a2d3-4ebe-ccea-8c0e4200d2e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[115., 108.,  92.],\n",
       "         [111., 105.,  89.],\n",
       "         [ 71.,  66.,  55.],\n",
       "         ...,\n",
       "         [106.,  89.,  76.],\n",
       "         [155., 135., 120.],\n",
       "         [159., 139., 124.]],\n",
       "\n",
       "        [[114., 107.,  91.],\n",
       "         [110., 104.,  88.],\n",
       "         [ 70.,  64.,  53.],\n",
       "         ...,\n",
       "         [105.,  88.,  76.],\n",
       "         [155., 135., 120.],\n",
       "         [159., 139., 124.]],\n",
       "\n",
       "        [[101.,  95.,  80.],\n",
       "         [ 98.,  92.,  77.],\n",
       "         [ 61.,  55.,  45.],\n",
       "         ...,\n",
       "         [ 97.,  81.,  68.],\n",
       "         [145., 127., 113.],\n",
       "         [150., 132., 117.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 47.,  32.,  23.],\n",
       "         [ 47.,  31.,  23.],\n",
       "         [ 41.,  28.,  19.],\n",
       "         ...,\n",
       "         [126., 119., 105.],\n",
       "         [127., 122., 105.],\n",
       "         [127., 122., 105.]],\n",
       "\n",
       "        [[ 49.,  35.,  25.],\n",
       "         [ 48.,  34.,  25.],\n",
       "         [ 42.,  28.,  19.],\n",
       "         ...,\n",
       "         [124., 116., 102.],\n",
       "         [124., 116., 100.],\n",
       "         [124., 116., 100.]],\n",
       "\n",
       "        [[ 50.,  35.,  25.],\n",
       "         [ 49.,  34.,  25.],\n",
       "         [ 42.,  28.,  19.],\n",
       "         ...,\n",
       "         [123., 116., 102.],\n",
       "         [123., 116., 100.],\n",
       "         [123., 116., 100.]]],\n",
       "\n",
       "\n",
       "       [[[ 11.,   4.,   0.],\n",
       "         [ 11.,   4.,   0.],\n",
       "         [ 12.,   6.,   2.],\n",
       "         ...,\n",
       "         [ 14.,   8.,   0.],\n",
       "         [ 16.,  11.,   2.],\n",
       "         [ 17.,  12.,   2.]],\n",
       "\n",
       "        [[ 11.,   4.,   0.],\n",
       "         [ 11.,   4.,   0.],\n",
       "         [ 12.,   6.,   2.],\n",
       "         ...,\n",
       "         [ 14.,   8.,   0.],\n",
       "         [ 16.,  12.,   2.],\n",
       "         [ 17.,  12.,   2.]],\n",
       "\n",
       "        [[ 11.,   5.,   1.],\n",
       "         [ 11.,   5.,   1.],\n",
       "         [ 13.,   7.,   4.],\n",
       "         ...,\n",
       "         [ 14.,   8.,   0.],\n",
       "         [ 15.,  11.,   1.],\n",
       "         [ 16.,  11.,   1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 13.,  12.,   9.],\n",
       "         [ 13.,  12.,   9.],\n",
       "         [ 13.,  12.,   9.],\n",
       "         ...,\n",
       "         [ 12.,  13.,  10.],\n",
       "         [ 12.,  13.,  10.],\n",
       "         [ 12.,  13.,  10.]],\n",
       "\n",
       "        [[ 13.,  13.,  10.],\n",
       "         [ 13.,  13.,  10.],\n",
       "         [ 13.,  13.,  10.],\n",
       "         ...,\n",
       "         [ 13.,  13.,  10.],\n",
       "         [ 12.,  13.,  10.],\n",
       "         [ 12.,  13.,  10.]],\n",
       "\n",
       "        [[ 13.,  13.,  10.],\n",
       "         [ 13.,  13.,  10.],\n",
       "         [ 13.,  13.,  10.],\n",
       "         ...,\n",
       "         [ 13.,  13.,  10.],\n",
       "         [ 12.,  13.,  10.],\n",
       "         [ 12.,  13.,  10.]]],\n",
       "\n",
       "\n",
       "       [[[208., 199., 176.],\n",
       "         [207., 198., 174.],\n",
       "         [204., 192., 169.],\n",
       "         ...,\n",
       "         [105.,  88.,  66.],\n",
       "         [106.,  89.,  66.],\n",
       "         [106.,  89.,  66.]],\n",
       "\n",
       "        [[207., 199., 175.],\n",
       "         [207., 198., 174.],\n",
       "         [202., 191., 167.],\n",
       "         ...,\n",
       "         [106.,  88.,  67.],\n",
       "         [106.,  89.,  66.],\n",
       "         [106.,  89.,  66.]],\n",
       "\n",
       "        [[206., 193., 173.],\n",
       "         [205., 191., 171.],\n",
       "         [196., 184., 161.],\n",
       "         ...,\n",
       "         [111.,  95.,  71.],\n",
       "         [110.,  93.,  70.],\n",
       "         [110.,  93.,  70.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[184., 164., 134.],\n",
       "         [181., 161., 131.],\n",
       "         [166., 143., 111.],\n",
       "         ...,\n",
       "         [119.,  96.,  61.],\n",
       "         [153., 129.,  91.],\n",
       "         [158., 134.,  95.]],\n",
       "\n",
       "        [[194., 170., 142.],\n",
       "         [190., 165., 138.],\n",
       "         [164., 140., 109.],\n",
       "         ...,\n",
       "         [137., 111.,  77.],\n",
       "         [147., 122.,  86.],\n",
       "         [148., 124.,  87.]],\n",
       "\n",
       "        [[196., 171., 144.],\n",
       "         [192., 166., 139.],\n",
       "         [164., 139., 110.],\n",
       "         ...,\n",
       "         [141., 114.,  80.],\n",
       "         [146., 121.,  84.],\n",
       "         [147., 122.,  85.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         ...,\n",
       "         [ 18.,  36.,  54.],\n",
       "         [ 36.,  54.,  72.],\n",
       "         [ 36.,  54.,  72.]],\n",
       "\n",
       "        [[ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         ...,\n",
       "         [ 18.,  54.,  54.],\n",
       "         [ 36.,  54.,  72.],\n",
       "         [ 36.,  54.,  72.]],\n",
       "\n",
       "        [[ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         ...,\n",
       "         [ 36.,  54.,  54.],\n",
       "         [ 36.,  54.,  72.],\n",
       "         [ 36.,  54.,  72.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         ...,\n",
       "         [ 54.,  54.,  91.],\n",
       "         [ 54.,  54.,  91.],\n",
       "         [ 54.,  54.,  91.]],\n",
       "\n",
       "        [[ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         ...,\n",
       "         [ 54.,  54.,  91.],\n",
       "         [ 54.,  54.,  91.],\n",
       "         [ 54.,  54.,  91.]],\n",
       "\n",
       "        [[ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         [ 36.,  36.,  54.],\n",
       "         ...,\n",
       "         [ 54.,  54.,  91.],\n",
       "         [ 54.,  54.,  91.],\n",
       "         [ 54.,  54.,  91.]]],\n",
       "\n",
       "\n",
       "       [[[ 33.,  26.,  25.],\n",
       "         [ 33.,  26.,  25.],\n",
       "         [ 35.,  28.,  26.],\n",
       "         ...,\n",
       "         [ 40.,  31.,  31.],\n",
       "         [ 43.,  31.,  31.],\n",
       "         [ 43.,  32.,  32.]],\n",
       "\n",
       "        [[ 33.,  26.,  25.],\n",
       "         [ 33.,  26.,  25.],\n",
       "         [ 35.,  28.,  26.],\n",
       "         ...,\n",
       "         [ 39.,  30.,  30.],\n",
       "         [ 42.,  31.,  31.],\n",
       "         [ 43.,  31.,  31.]],\n",
       "\n",
       "        [[ 33.,  28.,  26.],\n",
       "         [ 33.,  28.,  26.],\n",
       "         [ 35.,  29.,  27.],\n",
       "         ...,\n",
       "         [ 39.,  29.,  29.],\n",
       "         [ 40.,  31.,  31.],\n",
       "         [ 41.,  32.,  32.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 53.,  39.,  31.],\n",
       "         [ 53.,  39.,  31.],\n",
       "         [ 52.,  38.,  31.],\n",
       "         ...,\n",
       "         [ 72.,  57.,  47.],\n",
       "         [ 71.,  56.,  47.],\n",
       "         [ 70.,  56.,  46.]],\n",
       "\n",
       "        [[ 52.,  38.,  32.],\n",
       "         [ 52.,  38.,  31.],\n",
       "         [ 52.,  38.,  32.],\n",
       "         ...,\n",
       "         [ 75.,  61.,  48.],\n",
       "         [ 72.,  59.,  48.],\n",
       "         [ 72.,  59.,  47.]],\n",
       "\n",
       "        [[ 52.,  38.,  31.],\n",
       "         [ 52.,  38.,  31.],\n",
       "         [ 52.,  38.,  31.],\n",
       "         ...,\n",
       "         [ 75.,  61.,  50.],\n",
       "         [ 73.,  59.,  48.],\n",
       "         [ 73.,  59.,  48.]]],\n",
       "\n",
       "\n",
       "       [[[ 46.,  44.,  55.],\n",
       "         [ 46.,  44.,  55.],\n",
       "         [ 42.,  40.,  51.],\n",
       "         ...,\n",
       "         [ 68.,  68.,  80.],\n",
       "         [ 71.,  71.,  79.],\n",
       "         [ 71.,  71.,  80.]],\n",
       "\n",
       "        [[ 46.,  44.,  55.],\n",
       "         [ 46.,  43.,  55.],\n",
       "         [ 42.,  40.,  51.],\n",
       "         ...,\n",
       "         [ 69.,  69.,  80.],\n",
       "         [ 71.,  71.,  80.],\n",
       "         [ 72.,  72.,  80.]],\n",
       "\n",
       "        [[ 44.,  42.,  53.],\n",
       "         [ 44.,  42.,  53.],\n",
       "         [ 41.,  40.,  50.],\n",
       "         ...,\n",
       "         [ 72.,  71.,  80.],\n",
       "         [ 71.,  71.,  81.],\n",
       "         [ 72.,  71.,  82.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 87.,  72.,  76.],\n",
       "         [ 86.,  71.,  75.],\n",
       "         [ 75.,  65.,  68.],\n",
       "         ...,\n",
       "         [ 47.,  34.,  38.],\n",
       "         [ 68.,  53.,  60.],\n",
       "         [ 72.,  57.,  63.]],\n",
       "\n",
       "        [[ 85.,  72.,  72.],\n",
       "         [ 84.,  70.,  71.],\n",
       "         [ 79.,  65.,  70.],\n",
       "         ...,\n",
       "         [ 50.,  36.,  42.],\n",
       "         [ 68.,  53.,  57.],\n",
       "         [ 70.,  55.,  60.]],\n",
       "\n",
       "        [[ 85.,  72.,  72.],\n",
       "         [ 84.,  71.,  71.],\n",
       "         [ 79.,  65.,  70.],\n",
       "         ...,\n",
       "         [ 51.,  37.,  42.],\n",
       "         [ 67.,  52.,  57.],\n",
       "         [ 69.,  54.,  59.]]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRI9s-aYdCkR"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We load the CIFAR-100 dataset through `tf.keras.datasets`,\n",
    "normalize the images, and convert the integer labels to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UC3t-dCHdCkR"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (224,224,3)\n",
    "\n",
    "# # # (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "# X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "# Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "# Y_test = keras.utils.to_categorical(Y_test, num_classes)\n",
    "# # print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "# # print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(25):\n",
    "#     plt.subplot(5, 5, i + 1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(X_train[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5cALzRqZ700m",
    "outputId": "f1994d44-276c-440c-c8d9-d94e2c572031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[238., 196., 206.],\n",
       "          [235., 193., 203.],\n",
       "          [232., 193., 203.],\n",
       "          ...,\n",
       "          [229., 187., 196.],\n",
       "          [229., 187., 196.],\n",
       "          [229., 187., 196.]],\n",
       " \n",
       "         [[235., 193., 203.],\n",
       "          [235., 193., 203.],\n",
       "          [232., 193., 200.],\n",
       "          ...,\n",
       "          [229., 187., 196.],\n",
       "          [229., 187., 196.],\n",
       "          [229., 187., 196.]],\n",
       " \n",
       "         [[235., 193., 203.],\n",
       "          [235., 193., 203.],\n",
       "          [232., 193., 200.],\n",
       "          ...,\n",
       "          [229., 187., 196.],\n",
       "          [229., 187., 196.],\n",
       "          [229., 187., 196.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[232., 190., 193.],\n",
       "          [232., 190., 193.],\n",
       "          [232., 190., 193.],\n",
       "          ...,\n",
       "          [232., 190., 196.],\n",
       "          [232., 190., 196.],\n",
       "          [232., 190., 196.]],\n",
       " \n",
       "         [[232., 190., 193.],\n",
       "          [232., 190., 193.],\n",
       "          [232., 190., 193.],\n",
       "          ...,\n",
       "          [232., 190., 196.],\n",
       "          [232., 190., 196.],\n",
       "          [232., 190., 196.]],\n",
       " \n",
       "         [[232., 190., 193.],\n",
       "          [232., 190., 193.],\n",
       "          [232., 190., 193.],\n",
       "          ...,\n",
       "          [232., 190., 196.],\n",
       "          [232., 190., 196.],\n",
       "          [232., 190., 196.]]],\n",
       " \n",
       " \n",
       "        [[[ 50.,  50.,  40.],\n",
       "          [ 50.,  50.,  40.],\n",
       "          [ 50.,  50.,  40.],\n",
       "          ...,\n",
       "          [ 65.,  65.,  50.],\n",
       "          [ 65.,  65.,  50.],\n",
       "          [ 65.,  65.,  50.]],\n",
       " \n",
       "         [[ 50.,  50.,  40.],\n",
       "          [ 50.,  50.,  40.],\n",
       "          [ 50.,  50.,  40.],\n",
       "          ...,\n",
       "          [ 65.,  65.,  50.],\n",
       "          [ 65.,  65.,  50.],\n",
       "          [ 65.,  65.,  50.]],\n",
       " \n",
       "         [[ 50.,  50.,  40.],\n",
       "          [ 50.,  50.,  40.],\n",
       "          [ 50.,  50.,  40.],\n",
       "          ...,\n",
       "          [ 65.,  65.,  50.],\n",
       "          [ 65.,  65.,  50.],\n",
       "          [ 65.,  65.,  50.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[235., 230., 200.],\n",
       "          [235., 230., 200.],\n",
       "          [235., 230., 200.],\n",
       "          ...,\n",
       "          [120.,  95.,  85.],\n",
       "          [110.,  90.,  80.],\n",
       "          [110.,  90.,  80.]],\n",
       " \n",
       "         [[235., 230., 200.],\n",
       "          [235., 230., 200.],\n",
       "          [235., 230., 200.],\n",
       "          ...,\n",
       "          [120.,  95.,  85.],\n",
       "          [115.,  95.,  80.],\n",
       "          [110.,  90.,  80.]],\n",
       " \n",
       "         [[235., 230., 200.],\n",
       "          [235., 230., 200.],\n",
       "          [235., 230., 200.],\n",
       "          ...,\n",
       "          [120.,  95.,  85.],\n",
       "          [120.,  95.,  80.],\n",
       "          [110.,  90.,  80.]]],\n",
       " \n",
       " \n",
       "        [[[ 21.,  21.,  19.],\n",
       "          [ 23.,  21.,  19.],\n",
       "          [ 23.,  21.,  19.],\n",
       "          ...,\n",
       "          [ 11.,  14.,  19.],\n",
       "          [  9.,  14.,  19.],\n",
       "          [  9.,  14.,  19.]],\n",
       " \n",
       "         [[ 21.,  21.,  19.],\n",
       "          [ 21.,  21.,  19.],\n",
       "          [ 21.,  21.,  19.],\n",
       "          ...,\n",
       "          [ 11.,  14.,  19.],\n",
       "          [  9.,  14.,  19.],\n",
       "          [  9.,  14.,  19.]],\n",
       " \n",
       "         [[ 21.,  21.,  19.],\n",
       "          [ 21.,  21.,  19.],\n",
       "          [ 21.,  21.,  19.],\n",
       "          ...,\n",
       "          [ 11.,  14.,  19.],\n",
       "          [  9.,  14.,  19.],\n",
       "          [  9.,  14.,  19.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 64.,  61.,  59.],\n",
       "          [ 59.,  54.,  54.],\n",
       "          [ 59.,  54.,  54.],\n",
       "          ...,\n",
       "          [147., 140., 133.],\n",
       "          [147., 138., 131.],\n",
       "          [145., 135., 128.]],\n",
       " \n",
       "         [[ 64.,  59.,  57.],\n",
       "          [ 59.,  54.,  54.],\n",
       "          [ 59.,  54.,  54.],\n",
       "          ...,\n",
       "          [147., 142., 135.],\n",
       "          [147., 142., 135.],\n",
       "          [147., 142., 135.]],\n",
       " \n",
       "         [[ 61.,  57.,  57.],\n",
       "          [ 59.,  54.,  54.],\n",
       "          [ 59.,  54.,  54.],\n",
       "          ...,\n",
       "          [147., 142., 135.],\n",
       "          [147., 142., 135.],\n",
       "          [147., 142., 135.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[231., 220., 226.],\n",
       "          [231., 220., 226.],\n",
       "          [231., 220., 226.],\n",
       "          ...,\n",
       "          [ 98.,  81.,  69.],\n",
       "          [ 92.,  69.,  57.],\n",
       "          [ 81.,  57.,  52.]],\n",
       " \n",
       "         [[231., 220., 226.],\n",
       "          [231., 220., 226.],\n",
       "          [231., 220., 226.],\n",
       "          ...,\n",
       "          [ 98.,  81.,  69.],\n",
       "          [ 92.,  69.,  57.],\n",
       "          [ 81.,  57.,  52.]],\n",
       " \n",
       "         [[231., 220., 226.],\n",
       "          [231., 220., 226.],\n",
       "          [231., 220., 226.],\n",
       "          ...,\n",
       "          [ 98.,  81.,  69.],\n",
       "          [ 92.,  69.,  57.],\n",
       "          [ 81.,  57.,  52.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 52.,  40.,  28.],\n",
       "          [ 52.,  40.,  28.],\n",
       "          [ 57.,  46.,  28.],\n",
       "          ...,\n",
       "          [ 40.,  28.,  28.],\n",
       "          [ 40.,  28.,  28.],\n",
       "          [ 40.,  28.,  28.]],\n",
       " \n",
       "         [[ 52.,  40.,  28.],\n",
       "          [ 52.,  40.,  28.],\n",
       "          [ 57.,  46.,  28.],\n",
       "          ...,\n",
       "          [ 34.,  23.,  28.],\n",
       "          [ 34.,  23.,  23.],\n",
       "          [ 34.,  23.,  23.]],\n",
       " \n",
       "         [[ 52.,  40.,  28.],\n",
       "          [ 52.,  40.,  28.],\n",
       "          [ 57.,  46.,  28.],\n",
       "          ...,\n",
       "          [ 34.,  23.,  28.],\n",
       "          [ 34.,  23.,  23.],\n",
       "          [ 34.,  23.,  23.]]],\n",
       " \n",
       " \n",
       "        [[[ 50.,  63.,  69.],\n",
       "          [ 50.,  63.,  69.],\n",
       "          [ 50.,  63.,  69.],\n",
       "          ...,\n",
       "          [ 64.,  51.,  32.],\n",
       "          [ 64.,  51.,  32.],\n",
       "          [ 64.,  51.,  32.]],\n",
       " \n",
       "         [[ 50.,  63.,  69.],\n",
       "          [ 50.,  63.,  69.],\n",
       "          [ 50.,  63.,  69.],\n",
       "          ...,\n",
       "          [ 64.,  51.,  32.],\n",
       "          [ 64.,  51.,  32.],\n",
       "          [ 64.,  51.,  32.]],\n",
       " \n",
       "         [[ 50.,  63.,  69.],\n",
       "          [ 50.,  63.,  69.],\n",
       "          [ 50.,  63.,  69.],\n",
       "          ...,\n",
       "          [ 64.,  51.,  32.],\n",
       "          [ 64.,  51.,  32.],\n",
       "          [ 64.,  51.,  32.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[192., 198., 186.],\n",
       "          [192., 198., 186.],\n",
       "          [192., 198., 186.],\n",
       "          ...,\n",
       "          [158., 161., 167.],\n",
       "          [158., 161., 167.],\n",
       "          [159., 162., 168.]],\n",
       " \n",
       "         [[192., 198., 186.],\n",
       "          [192., 198., 186.],\n",
       "          [192., 198., 186.],\n",
       "          ...,\n",
       "          [156., 158., 163.],\n",
       "          [156., 158., 163.],\n",
       "          [156., 158., 163.]],\n",
       " \n",
       "         [[192., 198., 186.],\n",
       "          [192., 198., 186.],\n",
       "          [192., 198., 186.],\n",
       "          ...,\n",
       "          [156., 158., 163.],\n",
       "          [156., 158., 163.],\n",
       "          [156., 158., 163.]]],\n",
       " \n",
       " \n",
       "        [[[190., 129.,  95.],\n",
       "          [190., 129.,  95.],\n",
       "          [190., 129.,  95.],\n",
       "          ...,\n",
       "          [ 17.,  17.,  17.],\n",
       "          [ 17.,  17.,  17.],\n",
       "          [ 17.,  17.,  17.]],\n",
       " \n",
       "         [[190., 129.,  95.],\n",
       "          [190., 129.,  95.],\n",
       "          [190., 129.,  95.],\n",
       "          ...,\n",
       "          [ 17.,  17.,  17.],\n",
       "          [ 17.,  17.,  17.],\n",
       "          [ 17.,  17.,  17.]],\n",
       " \n",
       "         [[190., 129.,  95.],\n",
       "          [190., 129.,  95.],\n",
       "          [190., 129.,  95.],\n",
       "          ...,\n",
       "          [ 17.,  17.,  17.],\n",
       "          [ 17.,  17.,  17.],\n",
       "          [ 17.,  17.,  17.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  8.,  25.,   0.],\n",
       "          [  8.,  25.,   0.],\n",
       "          [  8.,  25.,   0.],\n",
       "          ...,\n",
       "          [  0., 159., 233.],\n",
       "          [  0., 159., 233.],\n",
       "          [  0., 159., 233.]],\n",
       " \n",
       "         [[  0.,   8.,   0.],\n",
       "          [  0.,   8.,   0.],\n",
       "          [  0.,   8.,   0.],\n",
       "          ...,\n",
       "          [  0., 164., 237.],\n",
       "          [  0., 164., 237.],\n",
       "          [  0., 164., 237.]],\n",
       " \n",
       "         [[  0.,   4.,   0.],\n",
       "          [  0.,   4.,   0.],\n",
       "          [  0.,   4.,   0.],\n",
       "          ...,\n",
       "          [  0., 168., 242.],\n",
       "          [  0., 168., 242.],\n",
       "          [  0., 168., 242.]]]], dtype=float32),\n",
       " array([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5fXM7F9I700m"
   },
   "outputs": [],
   "source": [
    "# X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJoysGe4dCkS"
   },
   "source": [
    "## Configure the hyperparameters\n",
    "\n",
    "A key parameter to pick is the `patch_size`, the size of the input patches.\n",
    "In order to use each pixel as an individual input, you can set `patch_size` to `(1, 1)`.\n",
    "Below, we take inspiration from the original paper settings\n",
    "for training on ImageNet-1K, keeping most of the original settings for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kWw5V30j700n"
   },
   "outputs": [],
   "source": [
    "# Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KvOi49BCdCkS"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "input_shape = (224,224,3)\n",
    "\n",
    "patch_size = (2, 2)  # 2-by-2 sized patches\n",
    "dropout_rate = 0.03  # Dropout rate\n",
    "num_heads = 8  # Attention heads\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_mlp = 256  # MLP layer size\n",
    "qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "window_size = 2  # Size of attention window\n",
    "shift_size = 1  # Size of shifting window\n",
    "image_dimension = 224  # Initial image size\n",
    "\n",
    "num_patch_x = input_shape[0] // patch_size[0]\n",
    "num_patch_y = input_shape[1] // patch_size[1]\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_epochs = 4\n",
    "validation_split = 0.1\n",
    "weight_decay = 0.0001\n",
    "label_smoothing = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kB4bPYzWdCkT"
   },
   "source": [
    "## Helper functions\n",
    "\n",
    "We create two helper functions to help us get a sequence of\n",
    "patches from the image, merge patches, and apply dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "LSddv2QQdCkT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def window_partition(x, window_size):\n",
    "    _, height, width, channels = x.shape\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n",
    "    )\n",
    "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, height, width, channels):\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(\n",
    "        windows,\n",
    "        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n",
    "    )\n",
    "    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n",
    "    x = tf.reshape(x, shape=(-1, height, width, channels))\n",
    "    return x\n",
    "\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    def __init__(self, drop_prob=None, **kwargs):\n",
    "        super(DropPath, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = tf.shape(x)\n",
    "        batch_size = input_shape[0]\n",
    "        rank = x.shape.rank\n",
    "        shape = (batch_size,) + (1,) * (rank - 1)\n",
    "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n",
    "        path_mask = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qqj-28YVdCkU"
   },
   "source": [
    "## Window based multi-head self-attention\n",
    "\n",
    "Usually Transformers perform global self-attention, where the relationships between\n",
    "a token and all other tokens are computed. The global computation leads to quadratic\n",
    "complexity with respect to the number of tokens. Here, as the [original paper](https://arxiv.org/abs/2103.14030)\n",
    "suggests, we compute self-attention within local windows, in a non-overlapping manner.\n",
    "Global self-attention leads to quadratic computational complexity in the number of patches,\n",
    "whereas window-based self-attention leads to linear complexity and is easily scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YHqFkVfldCkU"
   },
   "outputs": [],
   "source": [
    "\n",
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(\n",
    "        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n",
    "    ):\n",
    "        super(WindowAttention, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_window_elements = (2 * self.window_size[0] - 1) * (\n",
    "            2 * self.window_size[1] - 1\n",
    "        )\n",
    "        self.relative_position_bias_table = self.add_weight(\n",
    "            shape=(num_window_elements, self.num_heads),\n",
    "            initializer=tf.initializers.Zeros(),\n",
    "            trainable=True,\n",
    "        )\n",
    "        coords_h = np.arange(self.window_size[0])\n",
    "        coords_w = np.arange(self.window_size[1])\n",
    "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n",
    "        coords = np.stack(coords_matrix)\n",
    "        coords_flatten = coords.reshape(2, -1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "\n",
    "        self.relative_position_index = tf.Variable(\n",
    "            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        _, size, channels = x.shape\n",
    "        head_dim = channels // self.num_heads\n",
    "        x_qkv = self.qkv(x)\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n",
    "        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n",
    "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
    "        q = q * self.scale\n",
    "        k = tf.transpose(k, perm=(0, 1, 3, 2))\n",
    "        attn = q @ k\n",
    "\n",
    "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
    "        relative_position_index_flat = tf.reshape(\n",
    "            self.relative_position_index, shape=(-1,)\n",
    "        )\n",
    "        relative_position_bias = tf.gather(\n",
    "            self.relative_position_bias_table, relative_position_index_flat\n",
    "        )\n",
    "        relative_position_bias = tf.reshape(\n",
    "            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n",
    "        )\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n",
    "        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.get_shape()[0]\n",
    "            mask_float = tf.cast(\n",
    "                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n",
    "            )\n",
    "            attn = (\n",
    "                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n",
    "                + mask_float\n",
    "            )\n",
    "            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        else:\n",
    "            attn = keras.activations.softmax(attn, axis=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x_qkv = attn @ v\n",
    "        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n",
    "        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPYtiHKEdCkV"
   },
   "source": [
    "## The complete Swin Transformer model\n",
    "\n",
    "Finally, we put together the complete Swin Transformer by replacing the standard multi-head\n",
    "attention (MHA) with shifted windows attention. As suggested in the\n",
    "original paper, we create a model comprising of a shifted window-based MHA\n",
    "layer, followed by a 2-layer MLP with GELU nonlinearity in between, applying\n",
    "`LayerNormalization` before each MSA layer and each MLP, and a residual\n",
    "connection after each of these layers.\n",
    "\n",
    "Notice that we only create a simple MLP with 2 Dense and\n",
    "2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is\n",
    "quite standard in the literature. However in this paper the authors use a\n",
    "2-layer MLP with GELU nonlinearity in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uUW4ws_hdCkV"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SwinTransformer(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_patch,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        num_mlp=1024,\n",
    "        qkv_bias=True,\n",
    "        dropout_rate=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(SwinTransformer, self).__init__(**kwargs)\n",
    "\n",
    "        self.dim = dim  # number of input dimensions\n",
    "        self.num_patch = num_patch  # number of embedded patches\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "        self.window_size = window_size  # size of window\n",
    "        self.shift_size = shift_size  # size of window shift\n",
    "        self.num_mlp = num_mlp  # number of MLP nodes\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=(self.window_size, self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        self.drop_path = DropPath(dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        self.mlp = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(num_mlp),\n",
    "                layers.Activation(keras.activations.gelu),\n",
    "                layers.Dropout(dropout_rate),\n",
    "                layers.Dense(dim),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if min(self.num_patch) < self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.num_patch)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.shift_size == 0:\n",
    "            self.attn_mask = None\n",
    "        else:\n",
    "            height, width = self.num_patch\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            mask_array = np.zeros((1, height, width, 1))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    mask_array[:, h, w, :] = count\n",
    "                    count += 1\n",
    "            mask_array = tf.convert_to_tensor(mask_array)\n",
    "\n",
    "            # mask array to windows\n",
    "            mask_windows = window_partition(mask_array, self.window_size)\n",
    "            mask_windows = tf.reshape(\n",
    "                mask_windows, shape=[-1, self.window_size * self.window_size]\n",
    "            )\n",
    "            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n",
    "                mask_windows, axis=2\n",
    "            )\n",
    "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
    "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
    "            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, num_patches_before, channels = x.shape\n",
    "        x_skip = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, shape=(-1, height, width, channels))\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(\n",
    "                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = tf.reshape(\n",
    "            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n",
    "        )\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        attn_windows = tf.reshape(\n",
    "            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n",
    "        )\n",
    "        shifted_x = window_reverse(\n",
    "            attn_windows, self.window_size, height, width, channels\n",
    "        )\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(\n",
    "                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = tf.reshape(x, shape=(-1, height * width, channels))\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        x_skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahxgwhCodCkW"
   },
   "source": [
    "## Model training and evaluation\n",
    "\n",
    "### Extract and embed patches\n",
    "\n",
    "We first create 3 layers to help us extract, embed and merge patches from the\n",
    "images on top of which we will later use the Swin Transformer class we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "sekUykGmdCkX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class PatchExtract(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super(PatchExtract, self).__init__(**kwargs)\n",
    "        self.patch_size_x = patch_size[0]\n",
    "        self.patch_size_y = patch_size[0]\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        print(batch_size)\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n",
    "            rates=(1, 1, 1, 1),\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dim = patches.shape[-1]\n",
    "        patch_num = patches.shape[1]\n",
    "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super(PatchEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n",
    "        return self.proj(patch) + self.pos_embed(pos)\n",
    "\n",
    "\n",
    "class PatchMerging(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, _, C = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape=(-1, height, width, C))\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = tf.concat((x0, x1, x2, x3), axis=-1)\n",
    "        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HmvByeUdCkY"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "We put together the Swin Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OQ3OTkxOdCkY",
    "outputId": "456a9691-84f9-4aad-9c71-151c76bfa0be",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"patch_extract/strided_slice:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(input_shape)\n",
    "x = layers.RandomCrop(image_dimension, image_dimension)(input)\n",
    "x = layers.RandomFlip(\"horizontal\")(x)\n",
    "x = PatchExtract(patch_size)(x)\n",
    "x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=0,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = SwinTransformer(\n",
    "    dim=embed_dim,\n",
    "    num_patch=(num_patch_x, num_patch_y),\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    "    shift_size=shift_size,\n",
    "    num_mlp=num_mlp,\n",
    "    qkv_bias=qkv_bias,\n",
    "    dropout_rate=dropout_rate,\n",
    ")(x)\n",
    "x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "output = layers.Dense(2, activation=\"softmax\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD8SSEqLdCkY"
   },
   "source": [
    "### Train on CIFAR-100\n",
    "\n",
    "We train the model on CIFAR-100. Here, we only train the model\n",
    "for 40 epochs to keep the training time short in this example.\n",
    "In practice, you should train for 150 epochs to reach convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "B5lymE9U700r"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SEbJWjyB700r"
   },
   "outputs": [],
   "source": [
    "K.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vjvkaZnm700r",
    "outputId": "65876edd-179e-4d65-ff2c-56f7c3926f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.76072653],\n",
       "         [0.36986434],\n",
       "         [0.75388344],\n",
       "         [0.66595854],\n",
       "         [0.58236829],\n",
       "         [0.41973727],\n",
       "         [0.86602442],\n",
       "         [0.06314855],\n",
       "         [0.46159059],\n",
       "         [0.35711686]],\n",
       "\n",
       "        [[0.34231832],\n",
       "         [0.94458578],\n",
       "         [0.27640333],\n",
       "         [0.20903406],\n",
       "         [0.42764898],\n",
       "         [0.43382483],\n",
       "         [0.49449449],\n",
       "         [0.17740065],\n",
       "         [0.78860726],\n",
       "         [0.32699598]],\n",
       "\n",
       "        [[0.77685121],\n",
       "         [0.70421099],\n",
       "         [0.62861883],\n",
       "         [0.46228389],\n",
       "         [0.41104782],\n",
       "         [0.70219687],\n",
       "         [0.30490436],\n",
       "         [0.37067577],\n",
       "         [0.37856858],\n",
       "         [0.17122771]],\n",
       "\n",
       "        [[0.84710302],\n",
       "         [0.53298385],\n",
       "         [0.327751  ],\n",
       "         [0.56396268],\n",
       "         [0.46887594],\n",
       "         [0.2763286 ],\n",
       "         [0.93828754],\n",
       "         [0.09937546],\n",
       "         [0.82674687],\n",
       "         [0.58202556]],\n",
       "\n",
       "        [[0.37004574],\n",
       "         [0.41284476],\n",
       "         [0.09483646],\n",
       "         [0.00382284],\n",
       "         [0.76237714],\n",
       "         [0.43825302],\n",
       "         [0.47982539],\n",
       "         [0.49533704],\n",
       "         [0.91303054],\n",
       "         [0.17949979]],\n",
       "\n",
       "        [[0.4851128 ],\n",
       "         [0.58853165],\n",
       "         [0.24484047],\n",
       "         [0.14739224],\n",
       "         [0.79516573],\n",
       "         [0.30855923],\n",
       "         [0.92750055],\n",
       "         [0.18569298],\n",
       "         [0.29595175],\n",
       "         [0.19980396]],\n",
       "\n",
       "        [[0.54756949],\n",
       "         [0.06777005],\n",
       "         [0.56583184],\n",
       "         [0.74629169],\n",
       "         [0.58282747],\n",
       "         [0.66745118],\n",
       "         [0.68245322],\n",
       "         [0.20555213],\n",
       "         [0.00345693],\n",
       "         [0.14343113]],\n",
       "\n",
       "        [[0.21652263],\n",
       "         [0.78157156],\n",
       "         [0.2500018 ],\n",
       "         [0.4526478 ],\n",
       "         [0.53491708],\n",
       "         [0.14553232],\n",
       "         [0.75051968],\n",
       "         [0.71042824],\n",
       "         [0.29858   ],\n",
       "         [0.13795637]],\n",
       "\n",
       "        [[0.73679934],\n",
       "         [0.98444108],\n",
       "         [0.183745  ],\n",
       "         [0.82906977],\n",
       "         [0.50222284],\n",
       "         [0.33100281],\n",
       "         [0.15016114],\n",
       "         [0.69403785],\n",
       "         [0.54961341],\n",
       "         [0.19003103]],\n",
       "\n",
       "        [[0.17714298],\n",
       "         [0.58885206],\n",
       "         [0.69585991],\n",
       "         [0.84508612],\n",
       "         [0.01315279],\n",
       "         [0.56350365],\n",
       "         [0.73195935],\n",
       "         [0.07296205],\n",
       "         [0.72427618],\n",
       "         [0.29089531]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = np.random.rand(1,10,10,1)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtdO87cE700r",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Y3FXz_f2700r"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OUNf_alv700s",
    "outputId": "7a288681-bd8f-4a3b-cef3-b3055d8895b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 224, 224, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZiSZMxQs700s",
    "outputId": "a6125fbb-3dea-45fd-ac63-c5ae0f4d5f29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[115., 108.,  92.],\n",
       "          [111., 105.,  89.],\n",
       "          [ 71.,  66.,  55.],\n",
       "          ...,\n",
       "          [106.,  89.,  76.],\n",
       "          [155., 135., 120.],\n",
       "          [159., 139., 124.]],\n",
       " \n",
       "         [[114., 107.,  91.],\n",
       "          [110., 104.,  88.],\n",
       "          [ 70.,  64.,  53.],\n",
       "          ...,\n",
       "          [105.,  88.,  76.],\n",
       "          [155., 135., 120.],\n",
       "          [159., 139., 124.]],\n",
       " \n",
       "         [[101.,  95.,  80.],\n",
       "          [ 98.,  92.,  77.],\n",
       "          [ 61.,  55.,  45.],\n",
       "          ...,\n",
       "          [ 97.,  81.,  68.],\n",
       "          [145., 127., 113.],\n",
       "          [150., 132., 117.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 47.,  32.,  23.],\n",
       "          [ 47.,  31.,  23.],\n",
       "          [ 41.,  28.,  19.],\n",
       "          ...,\n",
       "          [126., 119., 105.],\n",
       "          [127., 122., 105.],\n",
       "          [127., 122., 105.]],\n",
       " \n",
       "         [[ 49.,  35.,  25.],\n",
       "          [ 48.,  34.,  25.],\n",
       "          [ 42.,  28.,  19.],\n",
       "          ...,\n",
       "          [124., 116., 102.],\n",
       "          [124., 116., 100.],\n",
       "          [124., 116., 100.]],\n",
       " \n",
       "         [[ 50.,  35.,  25.],\n",
       "          [ 49.,  34.,  25.],\n",
       "          [ 42.,  28.,  19.],\n",
       "          ...,\n",
       "          [123., 116., 102.],\n",
       "          [123., 116., 100.],\n",
       "          [123., 116., 100.]]],\n",
       " \n",
       " \n",
       "        [[[ 11.,   4.,   0.],\n",
       "          [ 11.,   4.,   0.],\n",
       "          [ 12.,   6.,   2.],\n",
       "          ...,\n",
       "          [ 14.,   8.,   0.],\n",
       "          [ 16.,  11.,   2.],\n",
       "          [ 17.,  12.,   2.]],\n",
       " \n",
       "         [[ 11.,   4.,   0.],\n",
       "          [ 11.,   4.,   0.],\n",
       "          [ 12.,   6.,   2.],\n",
       "          ...,\n",
       "          [ 14.,   8.,   0.],\n",
       "          [ 16.,  12.,   2.],\n",
       "          [ 17.,  12.,   2.]],\n",
       " \n",
       "         [[ 11.,   5.,   1.],\n",
       "          [ 11.,   5.,   1.],\n",
       "          [ 13.,   7.,   4.],\n",
       "          ...,\n",
       "          [ 14.,   8.,   0.],\n",
       "          [ 15.,  11.,   1.],\n",
       "          [ 16.,  11.,   1.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 13.,  12.,   9.],\n",
       "          [ 13.,  12.,   9.],\n",
       "          [ 13.,  12.,   9.],\n",
       "          ...,\n",
       "          [ 12.,  13.,  10.],\n",
       "          [ 12.,  13.,  10.],\n",
       "          [ 12.,  13.,  10.]],\n",
       " \n",
       "         [[ 13.,  13.,  10.],\n",
       "          [ 13.,  13.,  10.],\n",
       "          [ 13.,  13.,  10.],\n",
       "          ...,\n",
       "          [ 13.,  13.,  10.],\n",
       "          [ 12.,  13.,  10.],\n",
       "          [ 12.,  13.,  10.]],\n",
       " \n",
       "         [[ 13.,  13.,  10.],\n",
       "          [ 13.,  13.,  10.],\n",
       "          [ 13.,  13.,  10.],\n",
       "          ...,\n",
       "          [ 13.,  13.,  10.],\n",
       "          [ 12.,  13.,  10.],\n",
       "          [ 12.,  13.,  10.]]],\n",
       " \n",
       " \n",
       "        [[[208., 199., 176.],\n",
       "          [207., 198., 174.],\n",
       "          [204., 192., 169.],\n",
       "          ...,\n",
       "          [105.,  88.,  66.],\n",
       "          [106.,  89.,  66.],\n",
       "          [106.,  89.,  66.]],\n",
       " \n",
       "         [[207., 199., 175.],\n",
       "          [207., 198., 174.],\n",
       "          [202., 191., 167.],\n",
       "          ...,\n",
       "          [106.,  88.,  67.],\n",
       "          [106.,  89.,  66.],\n",
       "          [106.,  89.,  66.]],\n",
       " \n",
       "         [[206., 193., 173.],\n",
       "          [205., 191., 171.],\n",
       "          [196., 184., 161.],\n",
       "          ...,\n",
       "          [111.,  95.,  71.],\n",
       "          [110.,  93.,  70.],\n",
       "          [110.,  93.,  70.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[184., 164., 134.],\n",
       "          [181., 161., 131.],\n",
       "          [166., 143., 111.],\n",
       "          ...,\n",
       "          [119.,  96.,  61.],\n",
       "          [153., 129.,  91.],\n",
       "          [158., 134.,  95.]],\n",
       " \n",
       "         [[194., 170., 142.],\n",
       "          [190., 165., 138.],\n",
       "          [164., 140., 109.],\n",
       "          ...,\n",
       "          [137., 111.,  77.],\n",
       "          [147., 122.,  86.],\n",
       "          [148., 124.,  87.]],\n",
       " \n",
       "         [[196., 171., 144.],\n",
       "          [192., 166., 139.],\n",
       "          [164., 139., 110.],\n",
       "          ...,\n",
       "          [141., 114.,  80.],\n",
       "          [146., 121.,  84.],\n",
       "          [147., 122.,  85.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          ...,\n",
       "          [ 18.,  36.,  54.],\n",
       "          [ 36.,  54.,  72.],\n",
       "          [ 36.,  54.,  72.]],\n",
       " \n",
       "         [[ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          ...,\n",
       "          [ 18.,  54.,  54.],\n",
       "          [ 36.,  54.,  72.],\n",
       "          [ 36.,  54.,  72.]],\n",
       " \n",
       "         [[ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          ...,\n",
       "          [ 36.,  54.,  54.],\n",
       "          [ 36.,  54.,  72.],\n",
       "          [ 36.,  54.,  72.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          ...,\n",
       "          [ 54.,  54.,  91.],\n",
       "          [ 54.,  54.,  91.],\n",
       "          [ 54.,  54.,  91.]],\n",
       " \n",
       "         [[ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          ...,\n",
       "          [ 54.,  54.,  91.],\n",
       "          [ 54.,  54.,  91.],\n",
       "          [ 54.,  54.,  91.]],\n",
       " \n",
       "         [[ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          [ 36.,  36.,  54.],\n",
       "          ...,\n",
       "          [ 54.,  54.,  91.],\n",
       "          [ 54.,  54.,  91.],\n",
       "          [ 54.,  54.,  91.]]],\n",
       " \n",
       " \n",
       "        [[[ 33.,  26.,  25.],\n",
       "          [ 33.,  26.,  25.],\n",
       "          [ 35.,  28.,  26.],\n",
       "          ...,\n",
       "          [ 40.,  31.,  31.],\n",
       "          [ 43.,  31.,  31.],\n",
       "          [ 43.,  32.,  32.]],\n",
       " \n",
       "         [[ 33.,  26.,  25.],\n",
       "          [ 33.,  26.,  25.],\n",
       "          [ 35.,  28.,  26.],\n",
       "          ...,\n",
       "          [ 39.,  30.,  30.],\n",
       "          [ 42.,  31.,  31.],\n",
       "          [ 43.,  31.,  31.]],\n",
       " \n",
       "         [[ 33.,  28.,  26.],\n",
       "          [ 33.,  28.,  26.],\n",
       "          [ 35.,  29.,  27.],\n",
       "          ...,\n",
       "          [ 39.,  29.,  29.],\n",
       "          [ 40.,  31.,  31.],\n",
       "          [ 41.,  32.,  32.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 53.,  39.,  31.],\n",
       "          [ 53.,  39.,  31.],\n",
       "          [ 52.,  38.,  31.],\n",
       "          ...,\n",
       "          [ 72.,  57.,  47.],\n",
       "          [ 71.,  56.,  47.],\n",
       "          [ 70.,  56.,  46.]],\n",
       " \n",
       "         [[ 52.,  38.,  32.],\n",
       "          [ 52.,  38.,  31.],\n",
       "          [ 52.,  38.,  32.],\n",
       "          ...,\n",
       "          [ 75.,  61.,  48.],\n",
       "          [ 72.,  59.,  48.],\n",
       "          [ 72.,  59.,  47.]],\n",
       " \n",
       "         [[ 52.,  38.,  31.],\n",
       "          [ 52.,  38.,  31.],\n",
       "          [ 52.,  38.,  31.],\n",
       "          ...,\n",
       "          [ 75.,  61.,  50.],\n",
       "          [ 73.,  59.,  48.],\n",
       "          [ 73.,  59.,  48.]]],\n",
       " \n",
       " \n",
       "        [[[ 46.,  44.,  55.],\n",
       "          [ 46.,  44.,  55.],\n",
       "          [ 42.,  40.,  51.],\n",
       "          ...,\n",
       "          [ 68.,  68.,  80.],\n",
       "          [ 71.,  71.,  79.],\n",
       "          [ 71.,  71.,  80.]],\n",
       " \n",
       "         [[ 46.,  44.,  55.],\n",
       "          [ 46.,  43.,  55.],\n",
       "          [ 42.,  40.,  51.],\n",
       "          ...,\n",
       "          [ 69.,  69.,  80.],\n",
       "          [ 71.,  71.,  80.],\n",
       "          [ 72.,  72.,  80.]],\n",
       " \n",
       "         [[ 44.,  42.,  53.],\n",
       "          [ 44.,  42.,  53.],\n",
       "          [ 41.,  40.,  50.],\n",
       "          ...,\n",
       "          [ 72.,  71.,  80.],\n",
       "          [ 71.,  71.,  81.],\n",
       "          [ 72.,  71.,  82.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 87.,  72.,  76.],\n",
       "          [ 86.,  71.,  75.],\n",
       "          [ 75.,  65.,  68.],\n",
       "          ...,\n",
       "          [ 47.,  34.,  38.],\n",
       "          [ 68.,  53.,  60.],\n",
       "          [ 72.,  57.,  63.]],\n",
       " \n",
       "         [[ 85.,  72.,  72.],\n",
       "          [ 84.,  70.,  71.],\n",
       "          [ 79.,  65.,  70.],\n",
       "          ...,\n",
       "          [ 50.,  36.,  42.],\n",
       "          [ 68.,  53.,  57.],\n",
       "          [ 70.,  55.,  60.]],\n",
       " \n",
       "         [[ 85.,  72.,  72.],\n",
       "          [ 84.,  71.,  71.],\n",
       "          [ 79.,  65.,  70.],\n",
       "          ...,\n",
       "          [ 51.,  37.,  42.],\n",
       "          [ 67.,  52.,  57.],\n",
       "          [ 69.,  54.,  59.]]]], dtype=float32),\n",
       " array([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], dtype=float32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EPB6DPcc700s",
    "outputId": "8a9843d7-146d-4b3f-dec9-de4ca324c8a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_1')>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "RD-w_akZ700s",
    "outputId": "e743bb7e-1675-4ce1-ec4b-c9e6bff54bb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'dense_10')>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WXRJCBm4700s"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = keras.Model(input, output)\n",
    "# model = Sequential()\n",
    "x,y = next(train_data)\n",
    "x,y = np.array(x,dtype=np.float32),np.array(y,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTZ0abT_dCkY",
    "outputId": "3d122d0d-90cd-4e9e-a74c-d5a45fc96fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/patch_extract/strided_slice:0\", shape=(), dtype=int32)\n",
      "Tensor(\"model/patch_extract/strided_slice:0\", shape=(), dtype=int32)\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/swin_transformer/sequential/activation/Gelu/truediv' defined at (most recent call last):\n    File \"D:\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\wuwul\\AppData\\Local\\Temp\\ipykernel_4040\\3729315926.py\", line 13, in <cell line: 13>\n      history = model.fit(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\wuwul\\AppData\\Local\\Temp\\ipykernel_4040\\358455029.py\", line 120, in call\n      x = self.mlp(x)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\core\\activation.py\", line 57, in call\n      return self.activation(inputs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\activations.py\", line 351, in gelu\n      return tf.nn.gelu(x, approximate)\nNode: 'model/swin_transformer/sequential/activation/Gelu/truediv'\nfailed to allocate memory\n\t [[{{node model/swin_transformer/sequential/activation/Gelu/truediv}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_6304]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      3\u001b[0m     loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(label_smoothing\u001b[38;5;241m=\u001b[39mlabel_smoothing),\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtfa\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     ],\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m     \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data\u001b[49m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'model/swin_transformer/sequential/activation/Gelu/truediv' defined at (most recent call last):\n    File \"D:\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\anaconda3\\envs\\tf\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2880, in run_cell\n      result = self._run_cell(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2935, in _run_cell\n      return runner(coro)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3134, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3337, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\wuwul\\AppData\\Local\\Temp\\ipykernel_4040\\3729315926.py\", line 13, in <cell line: 13>\n      history = model.fit(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\wuwul\\AppData\\Local\\Temp\\ipykernel_4040\\358455029.py\", line 120, in call\n      x = self.mlp(x)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n      return self._run_internal_graph(\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\layers\\core\\activation.py\", line 57, in call\n      return self.activation(inputs)\n    File \"D:\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\activations.py\", line 351, in gelu\n      return tf.nn.gelu(x, approximate)\nNode: 'model/swin_transformer/sequential/activation/Gelu/truediv'\nfailed to allocate memory\n\t [[{{node model/swin_transformer/sequential/activation/Gelu/truediv}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_6304]"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "model.compile(\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
    "    optimizer=tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    ),\n",
    "    metrics=[\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "#         keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "     batch_size=10,\n",
    "     epochs=num_epochs,\n",
    "    validation_data=test_data\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyhvfOfD700t"
   },
   "outputs": [],
   "source": [
    "# model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bXQQ6mYH700t",
    "outputId": "236b96c4-95c9-41c1-8d57-70719a212b92"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW1r5EhMdCkY"
   },
   "source": [
    "Let's visualize the training progress of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohcM3TM2dCkY",
    "outputId": "1940f09d-5fae-4f15-a7ef-49991b3b13d4"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDn8ISaHdCkZ"
   },
   "source": [
    "Let's display the final results of the training on CIFAR-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqcBv77edCkZ",
    "outputId": "c4fbfdd8-8e1b-4927-b55a-ebea30d62fb7"
   },
   "outputs": [],
   "source": [
    "loss, accuracy, top_5_accuracy = model.evaluate(train_data)\n",
    "print(f\"Test loss: {round(loss, 2)}\")\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_bt_HWu700t"
   },
   "outputs": [],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhPvI7lc700t"
   },
   "outputs": [],
   "source": [
    "model.evaluate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdETdDcSdCkZ"
   },
   "source": [
    "The Swin Transformer model we just trained has just 152K parameters, and it gets\n",
    "us to ~75% test top-5 accuracy within just 40 epochs without any signs of overfitting\n",
    "as well as seen in above graph. This means we can train this network for longer\n",
    "(perhaps with a bit more regularization) and obtain even better performance.\n",
    "This performance can further be improved by additional techniques like cosine\n",
    "decay learning rate schedule, other data augmentation techniques. While experimenting,\n",
    "I tried training the model for 150 epochs with a slightly higher dropout and greater\n",
    "embedding dimensions which pushes the performance to ~72% test accuracy on CIFAR-100\n",
    "as you can see in the screenshot.\n",
    "\n",
    "![Results of training for longer](https://i.imgur.com/9vnQesZ.png)\n",
    "\n",
    "The authors present a top-1 accuracy of 87.3% on ImageNet. The authors also present\n",
    "a number of experiments to study how input sizes, optimizers etc. affect the final\n",
    "performance of this model. The authors further present using this model for object detection,\n",
    "semantic segmentation and instance segmentation as well and report competitive results\n",
    "for these. You are strongly advised to also check out the\n",
    "[original paper](https://arxiv.org/abs/2103.14030).\n",
    "\n",
    "This example takes inspiration from the official\n",
    "[PyTorch](https://github.com/microsoft/Swin-Transformer) and\n",
    "[TensorFlow](https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow) implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyrqC2dM700u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvZeQilc700u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "41Oyq_qZ700u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOh3xUaK700u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "swin_transformers (1) (1) (3) (3).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
